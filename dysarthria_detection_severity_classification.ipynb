{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Models for Detection and Severity Level Classification of Dysarthria from Speech\n",
    "\n",
    "## Paper Implementation\n",
    "\n",
    "**Authors:** Farhad Javanmardi, Sudarsana Reddy Kadiri, Paavo Alku  \n",
    "**Published in:** Speech Communication, Volume 158, 2024  \n",
    "**DOI:** https://doi.org/10.1016/j.specom.2024.103047\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Automatic detection and severity level classification of dysarthria from speech enables non-invasive and effective diagnosis that helps clinical decisions about medication and therapy of patients. In this work, three pre-trained models (**wav2vec2-BASE**, **wav2vec2-LARGE**, and **HuBERT**) are studied to extract features to build automatic detection and severity level classification systems for dysarthric speech.\n",
    "\n",
    "The experiments were conducted using two publicly available databases:\n",
    "- **UA-Speech**: 15 dysarthric speakers + 13 control speakers\n",
    "- **TORGO**: 8 dysarthric speakers + 7 control speakers\n",
    "\n",
    "**Classifiers used:**\n",
    "1. Support Vector Machine (SVM)\n",
    "2. Convolutional Neural Network (CNN)\n",
    "\n",
    "**Baseline features compared:**\n",
    "- MFCCs (Mel-Frequency Cepstral Coefficients)\n",
    "- openSMILE (ComParE 2016)\n",
    "- eGeMAPS (extended Geneva Minimalistic Acoustic Parameter Set)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#1-setup-and-imports)\n",
    "2. [Configuration](#2-configuration)\n",
    "3. [Data Loading](#3-data-loading)\n",
    "4. [Feature Extraction](#4-feature-extraction)\n",
    "   - 4.1 [MFCC Features](#41-mfcc-features)\n",
    "   - 4.2 [openSMILE Features](#42-opensmile-features)\n",
    "   - 4.3 [eGeMAPS Features](#43-egemaps-features)\n",
    "   - 4.4 [wav2vec2-BASE Features](#44-wav2vec2-base-features)\n",
    "   - 4.5 [wav2vec2-LARGE Features](#45-wav2vec2-large-features)\n",
    "   - 4.6 [HuBERT Features](#46-hubert-features)\n",
    "5. [Classification Models](#5-classification-models)\n",
    "   - 5.1 [SVM Classifier](#51-svm-classifier)\n",
    "   - 5.2 [CNN Classifier](#52-cnn-classifier)\n",
    "6. [Experiments](#6-experiments)\n",
    "   - 6.1 [Detection Task](#61-detection-task)\n",
    "   - 6.2 [Severity Classification Task](#62-severity-classification-task)\n",
    "7. [Results Analysis](#7-results-analysis)\n",
    "8. [Comparison with Paper Results](#8-comparison-with-paper-results)\n",
    "9. [Conclusions](#9-conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchaudio transformers librosa opensmile scikit-learn pandas numpy matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local modules\n",
    "from config import (\n",
    "    AudioConfig,\n",
    "    MFCCConfig,\n",
    "    OpenSMILEConfig,\n",
    "    EGeMAPSConfig,\n",
    "    Wav2Vec2Config,\n",
    "    HuBERTConfig,\n",
    "    SVMConfig,\n",
    "    CNNConfig,\n",
    "    UASpeechConfig,\n",
    "    TORGOConfig,\n",
    "    ExperimentConfig,\n",
    "    TaskType,\n",
    "    ClassifierType,\n",
    "    FeatureType,\n",
    "    PRETRAINED_MODELS,\n",
    "    PAPER_RESULTS,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    # Audio processing\n",
    "    load_audio,\n",
    "    preprocess_audio_batch,\n",
    "    \n",
    "    # Feature extraction\n",
    "    extract_mfcc,\n",
    "    extract_mfcc_stats,\n",
    "    extract_opensmile_features,\n",
    "    extract_egemaps_features,\n",
    "    PretrainedFeatureExtractor,\n",
    "    extract_wav2vec2_base_features,\n",
    "    extract_wav2vec2_large_features,\n",
    "    extract_hubert_features,\n",
    "    extract_features,\n",
    "    extract_features_batch,\n",
    "    \n",
    "    # Dataset classes\n",
    "    UASpeechDataset,\n",
    "    TORGODataset,\n",
    "    create_synthetic_dataset,\n",
    "    \n",
    "    # Classifiers\n",
    "    SVMClassifier,\n",
    "    CNNClassifier,\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluate_classifier,\n",
    "    get_confusion_matrix,\n",
    "    print_classification_report,\n",
    "    leave_one_speaker_out_cv,\n",
    "    run_experiment,\n",
    "    run_all_experiments,\n",
    "    \n",
    "    # Visualization\n",
    "    plot_confusion_matrix,\n",
    "    plot_results_comparison,\n",
    "    plot_training_history,\n",
    "    \n",
    "    # Utilities\n",
    "    save_results,\n",
    "    load_results,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Audio configuration\n",
    "audio_config = AudioConfig(\n",
    "    sample_rate=16000,      # Standard for speech processing\n",
    "    normalize=True,         # Normalize audio amplitude\n",
    "    remove_silence=False,   # Keep original timing\n",
    "    min_duration=0.5,       # Minimum 0.5 seconds\n",
    "    max_duration=10.0,      # Maximum 10 seconds\n",
    ")\n",
    "\n",
    "print(\"Audio Configuration:\")\n",
    "print(f\"  Sample Rate: {audio_config.sample_rate} Hz\")\n",
    "print(f\"  Normalize: {audio_config.normalize}\")\n",
    "print(f\"  Min Duration: {audio_config.min_duration}s\")\n",
    "print(f\"  Max Duration: {audio_config.max_duration}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction configurations\n",
    "mfcc_config = MFCCConfig()\n",
    "opensmile_config = OpenSMILEConfig()\n",
    "egemaps_config = EGeMAPSConfig()\n",
    "wav2vec2_base_config = Wav2Vec2Config(model_name=\"facebook/wav2vec2-base\")\n",
    "wav2vec2_large_config = Wav2Vec2Config(model_name=\"facebook/wav2vec2-large\")\n",
    "hubert_config = HuBERTConfig()\n",
    "\n",
    "print(\"Feature Dimensions:\")\n",
    "print(f\"  MFCC: {mfcc_config.feature_dim} (13 MFCCs + deltas + delta-deltas)\")\n",
    "print(f\"  openSMILE ComParE: {opensmile_config.feature_dim}\")\n",
    "print(f\"  eGeMAPS: {egemaps_config.feature_dim}\")\n",
    "print(f\"  wav2vec2-BASE: {wav2vec2_base_config.hidden_size}\")\n",
    "print(f\"  wav2vec2-LARGE: {wav2vec2_large_config.hidden_size}\")\n",
    "print(f\"  HuBERT: {hubert_config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier configurations\n",
    "svm_config = SVMConfig()\n",
    "cnn_config = CNNConfig()\n",
    "\n",
    "print(\"SVM Configuration:\")\n",
    "print(f\"  Kernel: {svm_config.kernel}\")\n",
    "print(f\"  C range: {svm_config.C_range}\")\n",
    "print(f\"  Class weight: {svm_config.class_weight}\")\n",
    "\n",
    "print(\"\\nCNN Configuration:\")\n",
    "print(f\"  Batch size: {cnn_config.batch_size}\")\n",
    "print(f\"  Epochs: {cnn_config.epochs}\")\n",
    "print(f\"  Learning rate: {cnn_config.learning_rate}\")\n",
    "print(f\"  Dropout: {cnn_config.dropout_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configurations\n",
    "uaspeech_config = UASpeechConfig()\n",
    "torgo_config = TORGOConfig()\n",
    "\n",
    "print(\"UA-Speech Dataset:\")\n",
    "print(f\"  Dysarthric speakers: {uaspeech_config.dysarthric_speakers}\")\n",
    "print(f\"  Control speakers: {uaspeech_config.control_speakers}\")\n",
    "print(f\"  Total speakers: {uaspeech_config.total_speakers}\")\n",
    "print(f\"  Severity levels: {list(uaspeech_config.severity_levels.keys())}\")\n",
    "\n",
    "print(\"\\nTORGO Dataset:\")\n",
    "print(f\"  Dysarthric speakers: {torgo_config.dysarthric_speakers}\")\n",
    "print(f\"  Control speakers: {torgo_config.control_speakers}\")\n",
    "print(f\"  Total speakers: {torgo_config.total_speakers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "**UA-Speech Database:**\n",
    "- 15 dysarthric speakers (4 female, 11 male) with Cerebral Palsy\n",
    "- 13 control speakers (4 female, 9 male)\n",
    "- Severity levels based on intelligibility:\n",
    "  - Very Low (76-100% intelligible): 3 speakers\n",
    "  - Low (51-75% intelligible): 4 speakers\n",
    "  - Medium (26-50% intelligible): 6 speakers\n",
    "  - High severity (0-25% intelligible): 2 speakers\n",
    "\n",
    "**TORGO Database:**\n",
    "- 8 dysarthric speakers with various severity levels\n",
    "- 7 control speakers\n",
    "- Includes articulatory data (not used in this paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths - UPDATE THESE TO YOUR LOCAL PATHS\n",
    "UASPEECH_PATH = \"./data/UA-Speech\"  # Path to UA-Speech dataset\n",
    "TORGO_PATH = \"./data/TORGO\"         # Path to TORGO dataset\n",
    "\n",
    "# Check if datasets exist\n",
    "uaspeech_exists = os.path.exists(UASPEECH_PATH)\n",
    "torgo_exists = os.path.exists(TORGO_PATH)\n",
    "\n",
    "print(f\"UA-Speech dataset found: {uaspeech_exists}\")\n",
    "print(f\"TORGO dataset found: {torgo_exists}\")\n",
    "\n",
    "if not uaspeech_exists and not torgo_exists:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NOTE: Datasets not found. Using synthetic data for demonstration.\")\n",
    "    print(\"\")\n",
    "    print(\"To use real data, download the datasets from:\")\n",
    "    print(\"- UA-Speech: http://www.isle.illinois.edu/sst/data/UASpeech/\")\n",
    "    print(\"- TORGO: http://www.cs.toronto.edu/~complingweb/data/TORGO/torgo.html\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create synthetic data\n",
    "USE_SYNTHETIC_DATA = not (uaspeech_exists or torgo_exists)\n",
    "\n",
    "if USE_SYNTHETIC_DATA:\n",
    "    print(\"Creating synthetic dataset for demonstration...\")\n",
    "    \n",
    "    # Create synthetic data for detection task\n",
    "    detection_features, detection_labels, detection_speakers = create_synthetic_dataset(\n",
    "        n_samples=500,\n",
    "        n_features=768,\n",
    "        n_speakers=28,  # Similar to UA-Speech\n",
    "        task=TaskType.DETECTION,\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    # Create synthetic data for severity classification\n",
    "    severity_features, severity_labels, severity_speakers = create_synthetic_dataset(\n",
    "        n_samples=300,\n",
    "        n_features=768,\n",
    "        n_speakers=15,  # Only dysarthric speakers\n",
    "        task=TaskType.SEVERITY,\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    print(f\"Detection data: {detection_features.shape[0]} samples, {detection_features.shape[1]} features\")\n",
    "    print(f\"Severity data: {severity_features.shape[0]} samples, {severity_features.shape[1]} features\")\n",
    "    print(f\"Detection labels distribution: {np.bincount(detection_labels)}\")\n",
    "    print(f\"Severity labels distribution: {np.bincount(severity_labels)}\")\n",
    "else:\n",
    "    print(\"Loading real dataset...\")\n",
    "    if uaspeech_exists:\n",
    "        uaspeech_dataset = UASpeechDataset(UASPEECH_PATH)\n",
    "        audio_files, detection_labels, detection_speakers = uaspeech_dataset.get_data(TaskType.DETECTION)\n",
    "        _, severity_labels, severity_speakers = uaspeech_dataset.get_data(TaskType.SEVERITY)\n",
    "        print(f\"UA-Speech: {len(audio_files)} audio files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction\n",
    "\n",
    "This section demonstrates all feature extraction methods used in the paper:\n",
    "\n",
    "1. **Baseline Features:**\n",
    "   - MFCC (Mel-Frequency Cepstral Coefficients)\n",
    "   - openSMILE ComParE 2016\n",
    "   - eGeMAPS (extended Geneva Minimalistic Acoustic Parameter Set)\n",
    "\n",
    "2. **Pre-trained Model Features:**\n",
    "   - wav2vec2-BASE (768-dim, 12 transformer layers)\n",
    "   - wav2vec2-LARGE (1024-dim, 24 transformer layers)\n",
    "   - HuBERT-BASE (768-dim, 12 transformer layers)\n",
    "\n",
    "### Key Finding from Paper:\n",
    "- **First layer** features are best for **detection** task\n",
    "- **Last layer** features are best for **severity classification** task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MFCC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_mfcc_extraction(audio_signal, sr):\n",
    "    \"\"\"\n",
    "    Demonstrate MFCC feature extraction.\n",
    "    \"\"\"\n",
    "    config = MFCCConfig()\n",
    "    \n",
    "    # Extract frame-level MFCCs\n",
    "    mfcc_frames = extract_mfcc(audio_signal, sr, config)\n",
    "    \n",
    "    # Extract utterance-level statistics\n",
    "    mfcc_stats = extract_mfcc_stats(audio_signal, sr, config)\n",
    "    \n",
    "    print(\"MFCC Extraction:\")\n",
    "    print(f\"  Frame-level shape: {mfcc_frames.shape} (frames x features)\")\n",
    "    print(f\"  Utterance-level shape: {mfcc_stats.shape} (mean + std)\")\n",
    "    \n",
    "    return mfcc_frames, mfcc_stats\n",
    "\n",
    "# Generate example audio for demonstration\n",
    "example_audio = np.random.randn(16000 * 2)  # 2 seconds of noise\n",
    "mfcc_frames, mfcc_stats = demonstrate_mfcc_extraction(example_audio, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MFCCs\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot MFCCs\n",
    "im = axes[0].imshow(mfcc_frames.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_xlabel('Frame')\n",
    "axes[0].set_ylabel('MFCC Coefficient')\n",
    "axes[0].set_title('MFCC Features Over Time')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Plot statistics\n",
    "x = np.arange(len(mfcc_stats))\n",
    "n_features = len(mfcc_stats) // 2\n",
    "axes[1].bar(x[:n_features], mfcc_stats[:n_features], alpha=0.7, label='Mean')\n",
    "axes[1].bar(x[:n_features], mfcc_stats[n_features:], alpha=0.7, label='Std', bottom=mfcc_stats[:n_features])\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('MFCC Statistics (Mean + Std)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 openSMILE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_opensmile_extraction(audio_signal, sr):\n",
    "    \"\"\"\n",
    "    Demonstrate openSMILE feature extraction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import opensmile\n",
    "        \n",
    "        config = OpenSMILEConfig()\n",
    "        features = extract_opensmile_features(audio_signal, sr, config)\n",
    "        \n",
    "        print(\"openSMILE ComParE 2016:\")\n",
    "        print(f\"  Feature dimension: {len(features)}\")\n",
    "        print(f\"  Feature range: [{features.min():.4f}, {features.max():.4f}]\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"openSMILE not installed. Install with: pip install opensmile\")\n",
    "        return np.random.randn(6373)  # Synthetic features\n",
    "\n",
    "opensmile_features = demonstrate_opensmile_extraction(example_audio, 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 eGeMAPS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_egemaps_extraction(audio_signal, sr):\n",
    "    \"\"\"\n",
    "    Demonstrate eGeMAPS feature extraction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import opensmile\n",
    "        \n",
    "        config = EGeMAPSConfig()\n",
    "        features = extract_egemaps_features(audio_signal, sr, config)\n",
    "        \n",
    "        print(\"eGeMAPS v02:\")\n",
    "        print(f\"  Feature dimension: {len(features)}\")\n",
    "        print(f\"  Feature range: [{features.min():.4f}, {features.max():.4f}]\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"openSMILE not installed. Install with: pip install opensmile\")\n",
    "        return np.random.randn(88)  # Synthetic features\n",
    "\n",
    "egemaps_features = demonstrate_egemaps_extraction(example_audio, 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 wav2vec2-BASE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_wav2vec2_extraction(audio_signal, sr, model_name=\"facebook/wav2vec2-base\"):\n",
    "    \"\"\"\n",
    "    Demonstrate wav2vec2 feature extraction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        extractor = PretrainedFeatureExtractor(model_name)\n",
    "        \n",
    "        # Extract from different layers\n",
    "        first_layer = extractor.extract_features(audio_signal, sr, layer=1, pooling='mean')\n",
    "        last_layer = extractor.extract_features(audio_signal, sr, layer=-1, pooling='mean')\n",
    "        all_layers = extractor.extract_features(audio_signal, sr, return_all_layers=True)\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  First layer features: {first_layer.shape}\")\n",
    "        print(f\"  Last layer features: {last_layer.shape}\")\n",
    "        print(f\"  Number of layers: {len(all_layers)}\")\n",
    "        \n",
    "        return extractor, first_layer, last_layer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name}: {e}\")\n",
    "        print(\"Using synthetic features for demonstration\")\n",
    "        return None, np.random.randn(768), np.random.randn(768)\n",
    "\n",
    "# Note: This requires downloading the model (may take a while on first run)\n",
    "print(\"Loading wav2vec2-BASE model...\")\n",
    "wav2vec2_extractor, wav2vec2_first, wav2vec2_last = demonstrate_wav2vec2_extraction(\n",
    "    example_audio, 16000, \"facebook/wav2vec2-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 wav2vec2-LARGE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav2vec2-LARGE (uncomment to run - requires more memory)\n",
    "# print(\"Loading wav2vec2-LARGE model...\")\n",
    "# wav2vec2_large_extractor, wav2vec2_large_first, wav2vec2_large_last = demonstrate_wav2vec2_extraction(\n",
    "#     example_audio, 16000, \"facebook/wav2vec2-large\"\n",
    "# )\n",
    "\n",
    "print(\"wav2vec2-LARGE:\")\n",
    "print(\"  Hidden size: 1024\")\n",
    "print(\"  Transformer layers: 24\")\n",
    "print(\"  (Skipped loading to save memory - uncomment above to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 HuBERT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_hubert_extraction(audio_signal, sr):\n",
    "    \"\"\"\n",
    "    Demonstrate HuBERT feature extraction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        extractor = PretrainedFeatureExtractor(\"facebook/hubert-base-ls960\")\n",
    "        \n",
    "        # Extract from different layers\n",
    "        first_layer = extractor.extract_features(audio_signal, sr, layer=1, pooling='mean')\n",
    "        last_layer = extractor.extract_features(audio_signal, sr, layer=-1, pooling='mean')\n",
    "        \n",
    "        print(\"\\nHuBERT-BASE:\")\n",
    "        print(f\"  First layer features: {first_layer.shape}\")\n",
    "        print(f\"  Last layer features: {last_layer.shape}\")\n",
    "        \n",
    "        return extractor, first_layer, last_layer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading HuBERT: {e}\")\n",
    "        print(\"Using synthetic features for demonstration\")\n",
    "        return None, np.random.randn(768), np.random.randn(768)\n",
    "\n",
    "print(\"Loading HuBERT model...\")\n",
    "hubert_extractor, hubert_first, hubert_last = demonstrate_hubert_extraction(example_audio, 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Models\n",
    "\n",
    "The paper uses two classifiers:\n",
    "1. **SVM (Support Vector Machine)** with RBF kernel and grid search for hyperparameter tuning\n",
    "2. **CNN (Convolutional Neural Network)** with 3 convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_svm_classifier():\n",
    "    \"\"\"\n",
    "    Demonstrate SVM classifier with grid search.\n",
    "    \"\"\"\n",
    "    # Create synthetic data\n",
    "    X_train = np.random.randn(200, 768)\n",
    "    y_train = np.random.randint(0, 2, 200)\n",
    "    X_test = np.random.randn(50, 768)\n",
    "    y_test = np.random.randint(0, 2, 50)\n",
    "    \n",
    "    # Initialize and train\n",
    "    svm_config = SVMConfig()\n",
    "    svm = SVMClassifier(svm_config)\n",
    "    \n",
    "    print(\"Training SVM with grid search...\")\n",
    "    svm.fit(X_train, y_train, tune_hyperparameters=True)\n",
    "    \n",
    "    print(f\"\\nBest parameters: {svm.best_params}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = svm.score(X_train, y_train)\n",
    "    test_acc = svm.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return svm\n",
    "\n",
    "svm_demo = demonstrate_svm_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_cnn_classifier():\n",
    "    \"\"\"\n",
    "    Demonstrate CNN classifier.\n",
    "    \"\"\"\n",
    "    # Create synthetic data\n",
    "    X_train = np.random.randn(200, 768)\n",
    "    y_train = np.random.randint(0, 2, 200)\n",
    "    X_val = np.random.randn(50, 768)\n",
    "    y_val = np.random.randint(0, 2, 50)\n",
    "    X_test = np.random.randn(50, 768)\n",
    "    y_test = np.random.randint(0, 2, 50)\n",
    "    \n",
    "    # Configure CNN\n",
    "    cnn_config = CNNConfig(\n",
    "        epochs=20,  # Reduced for demo\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "    \n",
    "    # Initialize and train\n",
    "    cnn = CNNClassifier(\n",
    "        input_dim=768,\n",
    "        num_classes=2,\n",
    "        config=cnn_config,\n",
    "    )\n",
    "    \n",
    "    print(\"Training CNN...\")\n",
    "    cnn.fit(X_train, y_train, X_val, y_val, verbose=True)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = cnn.score(X_train, y_train)\n",
    "    test_acc = cnn.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"\\nTrain accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return cnn\n",
    "\n",
    "cnn_demo = demonstrate_cnn_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig = plot_training_history(cnn_demo.history, title=\"CNN Training History (Demo)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiments\n",
    "\n",
    "### Evaluation Protocol\n",
    "\n",
    "Following the paper, we use **Leave-One-Speaker-Out (LOSO)** cross-validation:\n",
    "- In each fold, one speaker is held out for testing\n",
    "- All remaining speakers are used for training\n",
    "- This ensures speaker-independent evaluation\n",
    "\n",
    "### Tasks\n",
    "1. **Detection Task**: Binary classification (Dysarthric vs. Control)\n",
    "2. **Severity Classification Task**: 4-class classification (Very Low, Low, Medium, High severity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Detection Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DETECTION TASK: Dysarthric vs. Control\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run detection experiments\n",
    "detection_results = {}\n",
    "\n",
    "# SVM with detection features\n",
    "print(\"\\n--- SVM Classifier ---\")\n",
    "svm_detection_results = leave_one_speaker_out_cv(\n",
    "    detection_features,\n",
    "    detection_labels,\n",
    "    detection_speakers,\n",
    "    classifier_type=ClassifierType.SVM,\n",
    "    verbose=True,\n",
    ")\n",
    "detection_results['SVM'] = svm_detection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN with detection features\n",
    "print(\"\\n--- CNN Classifier ---\")\n",
    "cnn_detection_results = leave_one_speaker_out_cv(\n",
    "    detection_features,\n",
    "    detection_labels,\n",
    "    detection_speakers,\n",
    "    classifier_type=ClassifierType.CNN,\n",
    "    classifier_config=CNNConfig(epochs=30),\n",
    "    verbose=True,\n",
    ")\n",
    "detection_results['CNN'] = cnn_detection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detection results\n",
    "print(\"\\nDetection Task Results:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for classifier_name, results in detection_results.items():\n",
    "    metrics = results['metrics']\n",
    "    print(f\"\\n{classifier_name}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"  Precision: {metrics['precision']*100:.2f}%\")\n",
    "    print(f\"  Recall: {metrics['recall']*100:.2f}%\")\n",
    "    print(f\"  F1 Score: {metrics['f1_score']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for detection\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, (name, results) in enumerate(detection_results.items()):\n",
    "    cm = results['confusion_matrix']\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=['Control', 'Dysarthric'],\n",
    "        yticklabels=['Control', 'Dysarthric'],\n",
    "        ax=axes[idx]\n",
    "    )\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('True')\n",
    "    axes[idx].set_title(f'{name} - Detection Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Severity Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SEVERITY CLASSIFICATION TASK\")\n",
    "print(\"Classes: Very Low, Low, Medium, High severity\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run severity classification experiments\n",
    "severity_results = {}\n",
    "\n",
    "# SVM with severity features\n",
    "print(\"\\n--- SVM Classifier ---\")\n",
    "svm_severity_results = leave_one_speaker_out_cv(\n",
    "    severity_features,\n",
    "    severity_labels,\n",
    "    severity_speakers,\n",
    "    classifier_type=ClassifierType.SVM,\n",
    "    verbose=True,\n",
    ")\n",
    "severity_results['SVM'] = svm_severity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN with severity features\n",
    "print(\"\\n--- CNN Classifier ---\")\n",
    "cnn_severity_results = leave_one_speaker_out_cv(\n",
    "    severity_features,\n",
    "    severity_labels,\n",
    "    severity_speakers,\n",
    "    classifier_type=ClassifierType.CNN,\n",
    "    classifier_config=CNNConfig(epochs=50),\n",
    "    verbose=True,\n",
    ")\n",
    "severity_results['CNN'] = cnn_severity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize severity results\n",
    "print(\"\\nSeverity Classification Results:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for classifier_name, results in severity_results.items():\n",
    "    metrics = results['metrics']\n",
    "    print(f\"\\n{classifier_name}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"  Precision: {metrics['precision']*100:.2f}%\")\n",
    "    print(f\"  Recall: {metrics['recall']*100:.2f}%\")\n",
    "    print(f\"  F1 Score: {metrics['f1_score']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for severity\n",
    "severity_labels_names = ['Very Low', 'Low', 'Medium', 'High']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (name, results) in enumerate(severity_results.items()):\n",
    "    cm = results['confusion_matrix']\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=severity_labels_names,\n",
    "        yticklabels=severity_labels_names,\n",
    "        ax=axes[idx]\n",
    "    )\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('True')\n",
    "    axes[idx].set_title(f'{name} - Severity Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "def compile_results(detection_results, severity_results):\n",
    "    \"\"\"\n",
    "    Compile all results into a DataFrame.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for classifier, results in detection_results.items():\n",
    "        rows.append({\n",
    "            'Task': 'Detection',\n",
    "            'Classifier': classifier,\n",
    "            'Accuracy': results['metrics']['accuracy'] * 100,\n",
    "            'Precision': results['metrics']['precision'] * 100,\n",
    "            'Recall': results['metrics']['recall'] * 100,\n",
    "            'F1 Score': results['metrics']['f1_score'] * 100,\n",
    "        })\n",
    "    \n",
    "    for classifier, results in severity_results.items():\n",
    "        rows.append({\n",
    "            'Task': 'Severity',\n",
    "            'Classifier': classifier,\n",
    "            'Accuracy': results['metrics']['accuracy'] * 100,\n",
    "            'Precision': results['metrics']['precision'] * 100,\n",
    "            'Recall': results['metrics']['recall'] * 100,\n",
    "            'F1 Score': results['metrics']['f1_score'] * 100,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "results_df = compile_results(detection_results, severity_results)\n",
    "print(\"\\nResults Summary:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Detection results\n",
    "detection_df = results_df[results_df['Task'] == 'Detection']\n",
    "x = np.arange(len(detection_df))\n",
    "axes[0].bar(x, detection_df['Accuracy'], color='steelblue', alpha=0.8)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(detection_df['Classifier'])\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].set_title('Detection Task')\n",
    "axes[0].set_ylim(0, 100)\n",
    "for i, v in enumerate(detection_df['Accuracy']):\n",
    "    axes[0].text(i, v + 1, f'{v:.1f}%', ha='center')\n",
    "\n",
    "# Severity results\n",
    "severity_df = results_df[results_df['Task'] == 'Severity']\n",
    "x = np.arange(len(severity_df))\n",
    "axes[1].bar(x, severity_df['Accuracy'], color='coral', alpha=0.8)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(severity_df['Classifier'])\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Severity Classification Task')\n",
    "axes[1].set_ylim(0, 100)\n",
    "for i, v in enumerate(severity_df['Accuracy']):\n",
    "    axes[1].text(i, v + 1, f'{v:.1f}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Paper Results\n",
    "\n",
    "The paper reports the following key findings:\n",
    "\n",
    "### Detection Task (UA-Speech, SVM)\n",
    "| Feature | Accuracy |\n",
    "|---------|----------|\n",
    "| MFCC | 95.24% |\n",
    "| openSMILE | 97.14% |\n",
    "| eGeMAPS | 96.19% |\n",
    "| wav2vec2-BASE | 98.10% |\n",
    "| wav2vec2-LARGE | 98.57% |\n",
    "| **HuBERT** | **100.00%** |\n",
    "\n",
    "### Severity Classification (UA-Speech, SVM)\n",
    "| Feature | Accuracy |\n",
    "|---------|----------|\n",
    "| MFCC | 52.38% |\n",
    "| openSMILE | 58.10% |\n",
    "| eGeMAPS | 59.05% |\n",
    "| wav2vec2-BASE | 64.76% |\n",
    "| wav2vec2-LARGE | 66.67% |\n",
    "| **HuBERT** | **69.51%** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display paper results\n",
    "print(\"Paper Results (from PAPER_RESULTS in config.py):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nDetection Task - UA-Speech:\")\n",
    "print(\"-\" * 40)\n",
    "for classifier in ['SVM', 'CNN']:\n",
    "    print(f\"\\n{classifier}:\")\n",
    "    for feature, acc in PAPER_RESULTS['detection']['UA-Speech'][classifier].items():\n",
    "        print(f\"  {feature}: {acc}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nSeverity Classification - UA-Speech:\")\n",
    "print(\"-\" * 40)\n",
    "for classifier in ['SVM', 'CNN']:\n",
    "    print(f\"\\n{classifier}:\")\n",
    "    for feature, acc in PAPER_RESULTS['severity']['UA-Speech'][classifier].items():\n",
    "        print(f\"  {feature}: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "def plot_paper_results():\n",
    "    \"\"\"\n",
    "    Visualize paper results.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    datasets = ['UA-Speech', 'TORGO']\n",
    "    tasks = ['detection', 'severity']\n",
    "    \n",
    "    for row, task in enumerate(tasks):\n",
    "        for col, dataset in enumerate(datasets):\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            features = list(PAPER_RESULTS[task][dataset]['SVM'].keys())\n",
    "            svm_accs = list(PAPER_RESULTS[task][dataset]['SVM'].values())\n",
    "            cnn_accs = list(PAPER_RESULTS[task][dataset]['CNN'].values())\n",
    "            \n",
    "            x = np.arange(len(features))\n",
    "            width = 0.35\n",
    "            \n",
    "            bars1 = ax.bar(x - width/2, svm_accs, width, label='SVM', color='steelblue')\n",
    "            bars2 = ax.bar(x + width/2, cnn_accs, width, label='CNN', color='coral')\n",
    "            \n",
    "            ax.set_ylabel('Accuracy (%)')\n",
    "            ax.set_title(f'{task.capitalize()} - {dataset}')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(features, rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.set_ylim(0, 105)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar in bars1:\n",
    "                height = bar.get_height()\n",
    "                ax.annotate(f'{height:.1f}',\n",
    "                           xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                           xytext=(0, 3),\n",
    "                           textcoords=\"offset points\",\n",
    "                           ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_paper_results()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### Key Findings from the Paper\n",
    "\n",
    "1. **Pre-trained models outperform baseline features**: Features extracted from wav2vec2 and HuBERT consistently outperform traditional acoustic features (MFCCs, openSMILE, eGeMAPS).\n",
    "\n",
    "2. **HuBERT performs best**: Among the three pre-trained models tested, HuBERT features achieve the highest accuracy for both detection and severity classification tasks.\n",
    "\n",
    "3. **Layer selection matters**:\n",
    "   - **First transformer layer** features are optimal for **detection** (binary classification)\n",
    "   - **Last transformer layer** features are optimal for **severity classification** (multi-class)\n",
    "\n",
    "4. **SVM vs CNN**: SVM classifier slightly outperforms CNN in most experiments, possibly due to the limited size of the datasets.\n",
    "\n",
    "### Improvements over Baselines\n",
    "\n",
    "**Detection Task (HuBERT vs best baseline):**\n",
    "- UA-Speech: +2.86% absolute improvement over openSMILE\n",
    "- TORGO: +1.33% absolute improvement over openSMILE\n",
    "\n",
    "**Severity Classification (HuBERT vs best baseline):**\n",
    "- UA-Speech: +10.46% absolute improvement over eGeMAPS\n",
    "- TORGO: +6.54% absolute improvement over eGeMAPS\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- Pre-trained speech models provide powerful features for clinical speech assessment\n",
    "- No fine-tuning is required - features can be extracted directly from frozen models\n",
    "- The approach enables non-invasive diagnosis support for dysarthria patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path(\"./results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "results_df.to_csv(output_dir / \"experiment_results.csv\", index=False)\n",
    "\n",
    "# Save detailed results as JSON\n",
    "all_results = {\n",
    "    'detection': {k: v['metrics'] for k, v in detection_results.items()},\n",
    "    'severity': {k: v['metrics'] for k, v in severity_results.items()},\n",
    "}\n",
    "save_results(all_results, output_dir / \"detailed_results.json\")\n",
    "\n",
    "print(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Javanmardi, F., Kadiri, S. R., & Alku, P. (2024). Pre-trained models for detection and severity level classification of dysarthria from speech. *Speech Communication*, 158, 103047.\n",
    "\n",
    "2. Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. *NeurIPS*.\n",
    "\n",
    "3. Hsu, W. N., Bolte, B., Tsai, Y. H. H., Lakhotia, K., Salakhutdinov, R., & Mohamed, A. (2021). HuBERT: Self-supervised speech representation learning by masked prediction of hidden units. *IEEE/ACM TASLP*.\n",
    "\n",
    "4. Kim, H., Hasegawa-Johnson, M., Perlman, A., Gunderson, J., Huang, T. S., Watkin, K., & Frame, S. (2008). Dysarthric speech database for universal access research. *Interspeech*.\n",
    "\n",
    "5. Rudzicz, F., Namasivayam, A. K., & Wolff, T. (2012). The TORGO database of acoustic and articulatory speech from speakers with dysarthria. *Language Resources and Evaluation*.\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook implements the paper: \"Pre-trained Models for Detection and Severity Level Classification of Dysarthria from Speech\" by Javanmardi et al. (2024)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
